"""
Flask web application for House Price Prediction.
"""

import os
import sys
import numpy as np
import pandas as pd
import joblib
from flask import Flask, render_template, request, jsonify
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
import base64
from io import BytesIO

# Add the project root to the Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# Import project modules
from src.data.loader import load_data
from src.features.engineering import complete_data_transformation
from src.models.train_evaluate import train_random_forest, evaluate_model, feature_importance

# Initialize Flask app
app = Flask(__name__)

# Global variables
MODEL_PATH = os.path.join(project_root, 'output', 'rf_model.pkl')
DATA_PATH = os.path.join(project_root, 'housing.csv')
# We train on log-transformed prices for more stable, realistic predictions
USE_LOG_TARGET = True


def train_and_save_model_if_needed():
    """
    Train and save the model if it doesn't exist.
    This keeps startup / page load fast after the first successful training.
    """
    os.makedirs(os.path.join(project_root, 'output'), exist_ok=True)

    if os.path.exists(MODEL_PATH):
        print("Model already exists â€“ skipping training.")
        return

    print("Training model (first-run)...")
    # Load and preprocess data
    data = load_data(DATA_PATH)
    # Split into train/test
    train_size = int(0.8 * len(data))
    train_data = data.iloc[:train_size]
    test_data = data.iloc[train_size:]

    # Transform data
    X_train, y_train, features = complete_data_transformation(train_data)
    # Apply log1p transform to target for more stable training
    if USE_LOG_TARGET and y_train is not None:
        y_train = np.log1p(y_train)

    # Train model
    model = train_random_forest(X_train, y_train)

    # Save model, features, and transformation info
    joblib.dump(model, MODEL_PATH)
    joblib.dump(features, os.path.join(project_root, 'output', 'features.pkl'))

    # Also save the unique categories for ocean_proximity to ensure consistent encoding
    ocean_prox_values = data['ocean_proximity'].unique().tolist()
    joblib.dump(ocean_prox_values, os.path.join(project_root, 'output', 'ocean_proximity_categories.pkl'))

    print("Model trained and saved.")


def load_trained_model():
    """Load the trained model and features."""
    model = joblib.load(MODEL_PATH)
    features = joblib.load(os.path.join(project_root, 'output', 'features.pkl'))
    return model, features


def transform_with_saved_features(dataframe, saved_features):
    """
    Run the feature engineering pipeline and align the output to the trained feature order.
    """
    transformed_array, labels, feature_names = complete_data_transformation(dataframe)
    transformed_df = pd.DataFrame(transformed_array, columns=feature_names)
    aligned_df = transformed_df.reindex(columns=saved_features, fill_value=0)
    return aligned_df.to_numpy(), labels


def fig_to_base64(fig):
    """Convert matplotlib figure to base64 string for HTML display."""
    buf = BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight')
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode('utf-8')
    buf.close()
    return img_str


def generate_feature_importance_plot(model, features):
    """Generate feature importance plot."""
    importances = feature_importance(model, features)
    top_features = importances.head(10)
    
    plt.figure(figsize=(10, 6))
    sns.barplot(x=top_features.values, y=top_features.index)
    plt.title('Top 10 Feature Importances')
    plt.tight_layout()
    
    img_str = fig_to_base64(plt.gcf())
    plt.close()
    return img_str


def generate_prediction_scatter_plot(actual, predicted):
    """Generate scatter plot of actual vs predicted values."""
    plt.figure(figsize=(8, 8))
    plt.scatter(actual, predicted, alpha=0.5)
    plt.xlabel('Actual House Prices')
    plt.ylabel('Predicted House Prices')
    plt.title('Actual vs Predicted House Prices')
    
    # Add diagonal line
    min_val = min(actual.min(), predicted.min())
    max_val = max(actual.max(), predicted.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--')
    
    plt.tight_layout()
    img_str = fig_to_base64(plt.gcf())
    plt.close()
    return img_str


def prepare_sample_data():
    """Prepare some sample data from the dataset."""
    data = load_data(DATA_PATH)
    return data.head(5).to_dict(orient='records')

def get_ocean_proximity_categories():
    """Get the ocean proximity categories used during training."""
    categories_path = os.path.join(project_root, 'output', 'ocean_proximity_categories.pkl')
    if os.path.exists(categories_path):
        return joblib.load(categories_path)
    else:
        # If not saved yet, use the ones from the original dataset
        data = load_data(DATA_PATH)
        return data['ocean_proximity'].unique().tolist()


@app.route('/')
def home():
    """Home page view."""
    # Ensure model is trained
    train_and_save_model_if_needed()
    
    # Load model and features
    model, saved_features = load_trained_model()
    
    # Generate feature importance plot
    feature_plot = generate_feature_importance_plot(model, saved_features)
    
    # Get sample data
    sample_data = prepare_sample_data()
    
    # Load data for visualization
    data = load_data(DATA_PATH)
    test_size = int(0.2 * len(data))
    test_data = data.iloc[:test_size]
    
    # Transform and align test data
    X_test, y_test = transform_with_saved_features(test_data, saved_features)
    
    # Make predictions on test data (invert log-transform if used)
    raw_preds = model.predict(X_test)
    if USE_LOG_TARGET:
        predictions = np.expm1(raw_preds)
    else:
        predictions = raw_preds
    # Generate predictions plot
    predictions_plot = generate_prediction_scatter_plot(y_test, predictions)
    
    # Get statistics
    from sklearn.metrics import mean_squared_error, r2_score
    rmse = np.sqrt(mean_squared_error(y_test, predictions))
    r2 = r2_score(y_test, predictions)
    
    # Get feature ranges for the form (use central percentiles to avoid extreme, unrealistic inputs)
    feature_ranges = {}
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col != 'median_house_value':
            col_series = data[col]
            feature_ranges[col] = {
                'min': float(col_series.quantile(0.05)),
                'max': float(col_series.quantile(0.95)),
                'mean': float(col_series.mean()),
                'default': float(col_series.median())
            }
    
    # Get ocean proximity categories - use saved categories to ensure consistency
    ocean_proximity_values = get_ocean_proximity_categories()

    price_stats = {
        'min': float(data['median_house_value'].min()),
        'max': float(data['median_house_value'].max()),
        'mean': float(data['median_house_value'].mean()),
        'median': float(data['median_house_value'].median())
    }
    
    return render_template(
        'index.html',
        feature_plot=feature_plot,
        predictions_plot=predictions_plot,
        sample_data=sample_data,
        rmse=rmse,
        r2=r2,
        feature_ranges=feature_ranges,
        ocean_proximity_values=ocean_proximity_values,
        price_stats=price_stats
    )


@app.route('/predict', methods=['POST'])
def predict():
    """Handle prediction requests."""
    # Get form data
    try:
        # Extract features from form
        features_dict = {}
        for field in request.form:
            if field != 'csrf_token':
                features_dict[field] = float(request.form[field]) if field != 'ocean_proximity' else request.form[field]
        
        # Create DataFrame with the input
        input_df = pd.DataFrame([features_dict])
        
        # Load model and features
        model, saved_features = load_trained_model()
        
        # Prepare dataset for consistent scaling/encoding
        data = load_data(DATA_PATH)
        feature_data = data.drop(columns=['median_house_value'])
        input_row = feature_data.iloc[0:1].copy()
        
        for col, value in features_dict.items():
            if col in input_row.columns:
                input_row[col] = value
        
        combined = pd.concat([feature_data, input_row], ignore_index=True)
        
        transformed_inputs, _ = transform_with_saved_features(combined, saved_features)
        X_input = transformed_inputs[-1:].reshape(1, -1)
        
        # Make prediction (invert log-transform if used)
        raw_pred = model.predict(X_input)[0]
        if USE_LOG_TARGET:
            prediction = np.expm1(raw_pred)
        else:
            prediction = raw_pred
        app.logger.debug("Predict request payload: %s", features_dict)
        app.logger.debug("Predict result: %s", prediction)
        
        # Return result
        return jsonify({
            'success': True,
            'prediction': float(prediction),
            'formatted_prediction': f"${prediction:,.2f}"
        })
    
    except Exception as e:
        import traceback
        traceback_str = traceback.format_exc()
        print(traceback_str)
        return jsonify({
            'success': False,
            'error': str(e)
        })


@app.route('/update_plots', methods=['POST'])
def update_plots():
    """Generate updated plots with new prediction included."""
    try:
        data = request.get_json()
        new_prediction = float(data.get('prediction', 0))
        
        # Load model and features
        model, saved_features = load_trained_model()
        
        # Load data for visualization
        data_df = load_data(DATA_PATH)
        test_size = int(0.2 * len(data_df))
        test_data = data_df.iloc[:test_size]
        
        # Transform and align test data
        X_test, y_test = transform_with_saved_features(test_data, saved_features)
        
        # Make predictions on test data (invert log-transform if used)
        raw_preds = model.predict(X_test)
        if USE_LOG_TARGET:
            predictions = np.expm1(raw_preds)
        else:
            predictions = raw_preds
        
        # Generate updated scatter plot with new prediction highlighted
        plt.figure(figsize=(8, 8))
        plt.scatter(y_test, predictions, alpha=0.5, label='Test Predictions', color='#5E72E4')
        
        # Add new prediction point (we'll use a dummy actual value for visualization)
        # Use the mean of test actuals as reference
        reference_actual = float(y_test.mean())
        plt.scatter([reference_actual], [new_prediction], 
                   s=200, color='#F97316', marker='*', 
                   label='Your Prediction', zorder=5, edgecolors='black', linewidths=2)
        
        # Add diagonal line
        min_val = min(y_test.min(), predictions.min(), new_prediction)
        max_val = max(y_test.max(), predictions.max(), new_prediction)
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Perfect Prediction')
        
        plt.xlabel('Actual House Prices', fontsize=12)
        plt.ylabel('Predicted House Prices', fontsize=12)
        plt.title('Actual vs Predicted House Prices (Updated)', fontsize=14, fontweight='bold')
        plt.legend(loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        updated_plot = fig_to_base64(plt.gcf())
        plt.close()
        
        # Generate updated feature importance plot (same as before, but fresh)
        feature_plot = generate_feature_importance_plot(model, saved_features)
        
        return jsonify({
            'success': True,
            'predictions_plot': updated_plot,
            'feature_plot': feature_plot
        })
    
    except Exception as e:
        import traceback
        traceback_str = traceback.format_exc()
        print(traceback_str)
        return jsonify({
            'success': False,
            'error': str(e)
        })


if __name__ == '__main__':
    app.run(debug=True, host='127.0.0.1', port=5000)